{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade pip\n",
    "# !pip install tensorflow-gpu==\"2.*\"\n",
    "# !pip install numpy pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Libraries & Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-15 12:28:37.070614: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-10.1/lib64\n",
      "2022-02-15 12:28:37.070632: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy.io import savemat\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.model_selection import cross_val_score, cross_validate\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report, accuracy_score, matthews_corrcoef, balanced_accuracy_score, precision_recall_fscore_support\n",
    "from sklearn.metrics import auc, average_precision_score, precision_recall_curve, roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, BatchNormalization, Dropout, Conv1D, Conv2D, MaxPooling2D, MaxPooling1D, Flatten\n",
    "from keras import optimizers\n",
    "from keras import metrics\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from random import sample\n",
    "\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dset(dset):\n",
    "    dset = pd.concat([pd.DataFrame(np.asarray(dset.conc > 0.01, dtype='int')).rename(columns={0:'label'}), \n",
    "           dset.iloc[:, 3:]], axis=1)\n",
    "    return dset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## raw data\n",
    "pbno3_b1_raw = pd.read_csv(\"data/pbno3_batch1.csv\")\n",
    "pbno3_b2_raw = pd.read_csv(\"data/pbno3_batch2.csv\")\n",
    "pbno3_b1_raw = prepare_dset(pbno3_b1_raw)\n",
    "pbno3_b2_raw = prepare_dset(pbno3_b2_raw)\n",
    "\n",
    "# ## Power Spectrum Normalized\n",
    "# pbno3_b1_psn = pd.read_csv(\"./data_final/pbno3_dset/new/pbno3_psn_batch1.csv\")\n",
    "# pbno3_b2_psn = pd.read_csv(\"./data_final/pbno3_dset/new/pbno3_psn_batch2.csv\")\n",
    "# pbno3_b1_psn = prepare_dset(pbno3_b1_psn)\n",
    "# pbno3_b2_psn = prepare_dset(pbno3_b2_psn)\n",
    "\n",
    "# ## Baseline Corrected\n",
    "# pbno3_b1_bc = pd.read_csv(\"./data_final/pbno3_dset/new/pbno3_bl_batch1.csv\")\n",
    "# pbno3_b2_bc = pd.read_csv(\"./data_final/pbno3_dset/new/pbno3_bl_batch2.csv\")\n",
    "# pbno3_b1_bc = prepare_dset(pbno3_b1_bc)\n",
    "# pbno3_b2_bc = prepare_dset(pbno3_b2_bc)\n",
    "\n",
    "# ## Batch Normalized\n",
    "# pbno3_b1_bn = pd.read_csv(\"./data_final/pbno3_dset/new/pbno3_bn_batch1.csv\")\n",
    "# pbno3_b2_bn = pd.read_csv(\"./data_final/pbno3_dset/new/pbno3_bn_batch2.csv\")\n",
    "# pbno3_b1_bn = prepare_dset(pbno3_b1_bn)\n",
    "# pbno3_b2_bn = prepare_dset(pbno3_b2_bn)\n",
    "\n",
    "# ## Baseline Correctd & Power Spectrum Normalized\n",
    "# pbno3_b1_bl_psn = pd.read_csv(\"./data_final/pbno3_dset/new/pbno3_bl_psn_batch1.csv\")\n",
    "# pbno3_b2_bl_psn = pd.read_csv(\"./data_final/pbno3_dset/new/pbno3_bl_psn_batch2.csv\")\n",
    "# pbno3_b1_bl_psn = prepare_dset(pbno3_b1_bl_psn)\n",
    "# pbno3_b2_bl_psn = prepare_dset(pbno3_b2_bl_psn)\n",
    "\n",
    "# ## Baseline Correctd & Batch Normalized\n",
    "# pbno3_b1_bl_bn = pd.read_csv(\"./data_final/pbno3_dset/new/pbno3_bl_bn_batch1.csv\")\n",
    "# pbno3_b2_bl_bn = pd.read_csv(\"./data_final/pbno3_dset/new/pbno3_bl_bn_batch2.csv\")\n",
    "# pbno3_b1_bl_bn = prepare_dset(pbno3_b1_bl_bn)\n",
    "# pbno3_b2_bl_bn = prepare_dset(pbno3_b2_bl_bn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Util Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define performance measures\n",
    "def yoden_index(y, y_pred):\n",
    "    tn, fp, fn, tp = confusion_matrix(y, y_pred, labels=[0,1]).ravel()\n",
    "    j = (tp/(tp+fn)) + (tn/(tn+fp)) - 1\n",
    "    return j\n",
    "\n",
    "def pmeasure(y, y_pred):\n",
    "    tn, fp, fn, tp = confusion_matrix(y, y_pred, labels=[0,1]).ravel()\n",
    "    sensitivity = tp / (tp + fn )\n",
    "    specificity = tn / (tn + fp)\n",
    "    f1score = (2 * tp) / (2 * tp + fp + fn)\n",
    "    return ({'Sensitivity': sensitivity, 'Specificity': specificity, 'F1-Score': f1score})\n",
    "\n",
    "def Show_Statistics(msg,mean_Stats, sd_Stats, sigfig):\n",
    "    print(msg.upper())\n",
    "    print(70*'-')\n",
    "    print('Accuracy:{} + {}'          .format(round(mean_Stats[0],sigfig), round(sd_Stats[0],sigfig)))\n",
    "    print('Sensitivity:{} + {} '      .format(round(mean_Stats[1],sigfig), round(sd_Stats[1],sigfig)))\n",
    "    print('Specificity:{} + {}'       .format(round(mean_Stats[2],sigfig), round(sd_Stats[2],sigfig)))\n",
    "    print('F1-Score:{} + {}'          .format(round(mean_Stats[3],sigfig), round(sd_Stats[3],sigfig)))\n",
    "    print('MCC:{} + {}'               .format(round(mean_Stats[4],sigfig), round(sd_Stats[4],sigfig)))\n",
    "    print('Balance Accuracy:{} + {}'  .format(round(mean_Stats[5],sigfig), round(sd_Stats[5],sigfig)))\n",
    "    print('Youden-Index:{} + {}'      .format(round(mean_Stats[6],sigfig), round(sd_Stats[6],sigfig)))\n",
    "    print('AUC:{} + {}'               .format(round(mean_Stats[7],sigfig), round(sd_Stats[7],sigfig)))\n",
    "    print('AUPR:{} + {}'              .format(round(mean_Stats[8],sigfig), round(sd_Stats[8],sigfig)))  \n",
    "    print(70*'-')\n",
    "\n",
    "def Calculate_Stats(y_actual,y_pred, y_score):\n",
    "    acc = accuracy_score(y_actual, y_pred)\n",
    "    sen = pmeasure(y_actual, y_pred)['Sensitivity']\n",
    "    spe = pmeasure(y_actual, y_pred)['Specificity']\n",
    "    f1 = pmeasure(y_actual, y_pred)['F1-Score']\n",
    "    mcc = matthews_corrcoef(y_actual, y_pred)\n",
    "    bacc = balanced_accuracy_score(y_actual, y_pred)\n",
    "    yi = yoden_index(y_actual, y_pred)\n",
    "    #auc = roc_auc_score(y_actual.argmax(axis=1), y_pred.argmax(axis=1))\n",
    "  \n",
    "    pre, rec, _ = precision_recall_curve(y_actual, y_score, pos_label=1)\n",
    "    fpr, tpr, _ = roc_curve(y_actual, y_score, pos_label=1)\n",
    "    auroc = auc(fpr, tpr)\n",
    "    aupr = auc(rec, pre)\n",
    "    \n",
    "    return acc, sen, spe, f1, mcc, bacc, yi, auroc, aupr  \n",
    "\n",
    "def label_by_th(y_pred, threshold=0.5):\n",
    "    y_pred_copy = y_pred.copy()\n",
    "    y_pred_copy[y_pred>= threshold] = 1 \n",
    "    y_pred_copy[y_pred<threshold] = 0 \n",
    "    return y_pred_copy\n",
    "\n",
    "def cutoff_youdens_j(fpr,tpr,thresholds):\n",
    "    j_scores = tpr-fpr\n",
    "    j_ordered = sorted(zip(j_scores,thresholds))\n",
    "    return j_ordered[-1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_perf_summary(dataset, model, type):\n",
    "    ## Prepare dataset\n",
    "    dataset.loc[dataset.label>=1,\"label\"]=1\n",
    "    \n",
    "    ## Datasets \n",
    "    [X, y_true] = np.array(dataset.iloc[:,1:]), np.array(dataset.iloc[:,0])\n",
    "    y_true = to_categorical(y_true)\n",
    "    if type==\"regression\":\n",
    "        y_score = model.predict(X, batch_size=1800, verbose=0)    \n",
    "    else:\n",
    "        y_score = model.predict(X, batch_size=1800, verbose=0)[:,1]\n",
    "    #y_score = model.predict(X,batch_size=1800, verbose=0)\n",
    "    #y_pred = to_categorical(get_label(y_score))      \n",
    "\n",
    "    # Optimal Threshold\n",
    "    fpr, tpr, thresholds_AUC = roc_curve(y_true.argmax(axis=1), y_score)\n",
    "    precision, recall, thresholds_AUPR = precision_recall_curve(y_true.argmax(axis=1),y_score)\n",
    "\n",
    "    ## Optimal Threshold metrics\n",
    "    distance = (1-fpr)**2+(1-tpr)**2\n",
    "    EERs = (1-recall)/(1-precision)\n",
    "    positive = sum(y_true.argmax(axis=1))\n",
    "    negative = y_true.shape[0]-positive\n",
    "    ratio = negative/positive\n",
    "    opt_t_AUC = thresholds_AUC[np.argmin(distance)]\n",
    "    opt_t_AUPR = thresholds_AUPR[np.argmin(np.abs(EERs-ratio))]\n",
    "    opt_yodens_j = cutoff_youdens_j(fpr, tpr, thresholds_AUC)        \n",
    "    y_pred_th = label_by_th(y_score, opt_yodens_j)\n",
    "    y_pred = to_categorical(y_pred_th)\n",
    "\n",
    "    print(confusion_matrix(y_true.argmax(axis=1), y_pred.argmax(axis=1), labels=[0,1]).ravel())    \n",
    "\n",
    "    ## Measures\n",
    "    t_acc, t_sen, t_spe, t_f1, t_mcc, t_bacc, t_yi, t_auc, t_aupr = Calculate_Stats(y_true, y_pred, y_score)\n",
    "\n",
    "    print(\"ACC: {}, Sen: {}, Spe: {}, F1: {}, MCC: {}, BACC: {}, YI: {}, AUC: {}, AUPR: {}\".format(t_acc, t_sen, t_spe, t_f1, t_mcc, t_bacc, t_yi, t_auc, t_aupr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scale_dset(dset):\n",
    "    res =[]\n",
    "    tmp_dset = dset.iloc[:,1:]\n",
    "    for i, row in tmp_dset.iterrows():\n",
    "        res.append(row/max(abs(row)))\n",
    "    \n",
    "    res = pd.DataFrame(res).reset_index(drop=True)\n",
    "    res = pd.concat([dset.iloc[:,0], res], axis=1)\n",
    "    \n",
    "    return res "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clf_res_ML(dev_dset, ind_test_dset, method):\n",
    "    num_Trials = 10    \n",
    "    dev_dset = get_scale_dset(dev_dset)\n",
    "    ind_test_dset = get_scale_dset(ind_test_dset)\n",
    "\n",
    "    # import warnings filter\n",
    "    from warnings import simplefilter\n",
    "    # ignore all future warnings\n",
    "    simplefilter(action='ignore', category=FutureWarning)\n",
    "    [DataX, LabelY] = np.array(dev_dset.iloc[:,1:]), np.array(dev_dset.iloc[:,0])    \n",
    "\n",
    "    Stats =[]\n",
    "    Stats_Ind =[]\n",
    "    pdb.set_trace()\n",
    "    for j in range(0, num_Trials):        \n",
    "        X_train, X_val, y_train, y_val =\\\n",
    "        train_test_split(DataX, LabelY, test_size=0.2, stratify = LabelY)\n",
    "        X_val, X_test, y_val, y_test =\\\n",
    "        train_test_split(X_val, y_val, test_size=0.5, stratify = y_val)\n",
    "\n",
    "        if method == \"LR\":\n",
    "            clf = LogisticRegression(random_state=0).fit(X_train, y_train)\n",
    "\n",
    "        elif method == \"LinSVM\":\n",
    "            clf = SVC(kernel = 'linear', probability=True).fit(X_train, y_train)\n",
    "\n",
    "        elif method == \"NB\":\n",
    "            clf = GaussianNB().fit(X_train, y_train)\n",
    "\n",
    "        elif method == \"DT\":\n",
    "            clf = DecisionTreeClassifier().fit(X_train, y_train)\n",
    "\n",
    "        elif method == \"MLP\":\n",
    "            clf = MLPClassifier().fit(X_train, y_train)\n",
    "\n",
    "        elif method == \"RF\":\n",
    "            clf = RandomForestClassifier().fit(X_train, y_train)\n",
    "\n",
    "        elif method == \"RBFSVM\":\n",
    "            clf = SVC(kernel = 'rbf', probability=True).fit(X_train, y_train)\n",
    "\n",
    "        else:\n",
    "            print(\"Please check avilable models\")\n",
    "            break;\n",
    "\n",
    "        y_train_pred = clf.predict(X_train)\n",
    "        y_train_score = clf.predict_proba(X_train)     \n",
    "        y_train_pred = to_categorical(y_train_score.argmax(axis=1))\n",
    "        y_train_score = y_train_score[:,1]\n",
    "\n",
    "        y_val_pred = clf.predict(X_val)\n",
    "        y_val_score = clf.predict_proba(X_val)     \n",
    "        y_val_pred = to_categorical(y_val_score.argmax(axis=1))\n",
    "        y_val_score = y_val_score[:,1]\n",
    "\n",
    "        y_test_pred = clf.predict(X_test)\n",
    "        y_test_score = clf.predict_proba(X_test) \n",
    "        y_test_pred = to_categorical(y_test_score.argmax(axis=1))\n",
    "        y_test_score = y_test_score[:,1] \n",
    "\n",
    "        print(confusion_matrix(y_test, y_test_pred.argmax(axis=1), labels=[0,1]).ravel())\n",
    "\n",
    "        y_train_pred = y_train_pred.argmax(axis=1)\n",
    "        y_val_pred = y_val_pred.argmax(axis=1)\n",
    "        y_test_pred = y_test_pred.argmax(axis=1)\n",
    "\n",
    "        ## Training Measures\n",
    "        tr_acc, tr_sen, tr_spe, tr_f1, tr_mcc, tr_bacc, tr_yi, tr_auc, tr_aupr = Calculate_Stats(y_train,y_train_pred, y_train_score);\n",
    "\n",
    "        ## Validation Measures\n",
    "        v_acc, v_sen, v_spe, v_f1, v_mcc, v_bacc, v_yi, v_auc, v_aupr = Calculate_Stats(y_val,y_val_pred, y_val_score);\n",
    "\n",
    "        ## Test Measures\n",
    "        t_acc, t_sen, t_spe, t_f1, t_mcc, t_bacc, t_yi, t_auc, t_aupr = Calculate_Stats(y_test,y_test_pred, y_test_score);\n",
    "\n",
    "        Stats.append([tr_acc, tr_sen, tr_spe, tr_f1, tr_mcc, tr_bacc, tr_yi, tr_auc, tr_aupr,\n",
    "                      v_acc, v_sen, v_spe, v_f1, v_mcc, v_bacc, v_yi, v_auc, v_aupr,\n",
    "                      t_acc, t_sen, t_spe, t_f1, t_mcc, t_bacc, t_yi, t_auc, t_aupr])  \n",
    "\n",
    "        print(' \\nTraining/ Validation / Test BACC :', tr_bacc,'/',v_bacc,'/',t_bacc,\n",
    "              ' \\nTraining/ Validation / Test Youden-index:', tr_yi,'/',v_yi,'/',t_yi,\n",
    "              ' \\nTraining/ Validation / Test MCC:', tr_mcc,'/',v_mcc,'/',t_mcc,\n",
    "              ' \\nTraining/ Validation / Test AUC:', tr_auc,'/',v_auc,'/',t_auc,\n",
    "              ' \\nTraining/ Validation / Test AUPR:', tr_aupr,'/',v_aupr,'/',t_aupr)    \n",
    "\n",
    "        ## Independent Test\n",
    "        [Xt1, Yt1] = np.array(ind_test_dset.iloc[:,1:]), np.array(ind_test_dset.iloc[:,0])\n",
    "\n",
    "        Yt1_pred = clf.predict(Xt1)\n",
    "        Yt1_score = clf.predict_proba(Xt1)\n",
    "\n",
    "        Stats_Ind.append(Calculate_Stats(Yt1, Yt1_pred, Yt1_score[:,1]))\n",
    "\n",
    "        print(Stats_Ind)\n",
    "    \n",
    "    return Stats, Stats_Ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_perf_mat(perf_list, model_type):\n",
    "    perf_tbl = pd.DataFrame(perf_list)\n",
    "    perf_tbl = pd.concat([perf_tbl, pd.DataFrame(np.repeat(model_type, len(perf_list)))], axis=1)\n",
    "    perf_tbl.columns = ['Accuracy', \"Sensitivity\", \"Specificity\", \"F1\", \"MCC\", \"BACC\", \n",
    "                        \"Yoden Index\", \"AUROC\", \"AUPR\", \"Model\"]    \n",
    "    return perf_tbl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_clf_res(devset, indset, species, prep_method):\n",
    "    # NB_cv, NB_ind = get_clf_res_ML(devset, indset, method='NB')\n",
    "    # DT_cv, DT_ind = get_clf_res_ML(devset, indset, method='DT')\n",
    "    # LR_cv, LR_ind = get_clf_res_ML(devset, indset, method='LR')\n",
    "    # RF_cv, RF_ind = get_clf_res_ML(devset, indset, method='RF')\n",
    "    # LinSVM_cv, LinSVM_ind = get_clf_res_ML(devset, indset, method='LinSVM')\n",
    "    # RBFSVM_cv, RBFSVM_ind = get_clf_res_ML(devset, indset, method='RBFSVM')\n",
    "    MLP_cv, MLP_ind = get_clf_res_ML(devset, indset, method='MLP')\n",
    "    \n",
    "    fold_ind_perf = pd.concat([\n",
    "                            # format_perf_mat(NB_ind, 'NB'), \n",
    "                            # format_perf_mat(DT_ind, 'DT'), \n",
    "                            # format_perf_mat(LR_ind, 'LR'),\n",
    "                            # format_perf_mat(RF_ind, 'RF'), \n",
    "                            # format_perf_mat(LinSVM_ind, 'LinSVM'),\n",
    "                            # format_perf_mat(RBFSVM_ind, 'RBFSVM'),\n",
    "                            format_perf_mat(MLP_ind, 'MLP')\n",
    "                               ], axis=0)\n",
    "\n",
    "    return fold_ind_perf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Evaluate Performance of Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MLP on batch 1...\n",
      "CM:  [ 500    0    0 1500]\n",
      "t_acc, t_sen, t_spe, t_f1, t_mcc, t_bacc, t_yi, t_auc, t_aupr:  1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0\n",
      "Testing trained MLP on batch 2...\n",
      "CM:  [494   6 991 509]\n",
      "t_acc, t_sen, t_spe, t_f1, t_mcc, t_bacc, t_yi, t_auc, t_aupr:  0.5015 0.3393333333333333 0.988 0.5052109181141439 0.32415596923550277 0.6636666666666666 0.32733333333333325 0.6034333333333333 0.871441820700748\n"
     ]
    }
   ],
   "source": [
    "# import warnings filter\n",
    "from warnings import simplefilter\n",
    "# ignore all future warnings\n",
    "simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# perf_raw_pbno3_1 = get_all_clf_res(pbno3_b1_raw, pbno3_b2_raw, 'pbno3_b1b2', 'raw')\n",
    "#perf_raw_pbno3_2 = get_all_clf_res(pbno3_b2_raw, pbno3_b1_raw, 'pbno3_b2b1', 'raw')\n",
    "\n",
    "#\n",
    "# var1, var2 = get_clf_res_ML(pbno3_b1_raw, pbno3_b2_raw, method='MLP')\n",
    "\n",
    "######################################################################\n",
    "print('Training MLP on batch 1...')\n",
    "dset = get_scale_dset(pbno3_b1_raw)\n",
    "[X_src, Y_src] = np.array(dset.iloc[:,1:]), np.array(dset.iloc[:,0])\n",
    "#\n",
    "clf = MLPClassifier().fit(X_src, Y_src)\n",
    "#\n",
    "y_test_pred = clf.predict(X_src)\n",
    "y_test_score = clf.predict_proba(X_src) \n",
    "y_test_pred = to_categorical(y_test_score.argmax(axis=1))\n",
    "y_test_pred = y_test_pred.argmax(axis=1)\n",
    "y_test_score = y_test_score[:,1] \n",
    "#\n",
    "print('CM: ', confusion_matrix(Y_src, y_test_pred, labels=[0,1]).ravel())\n",
    "## Test Measures\n",
    "t_acc, t_sen, t_spe, t_f1, t_mcc, t_bacc, t_yi, t_auc, t_aupr = Calculate_Stats(Y_src,y_test_pred, y_test_score)\n",
    "print('t_acc, t_sen, t_spe, t_f1, t_mcc, t_bacc, t_yi, t_auc, t_aupr: ', t_acc, t_sen, t_spe, t_f1, t_mcc, t_bacc, t_yi, t_auc, t_aupr )\n",
    "\n",
    "######################################################################\n",
    "print('Testing trained MLP on batch 2...')\n",
    "dset = get_scale_dset(pbno3_b2_raw)\n",
    "[X_src, Y_src] = np.array(dset.iloc[:,1:]), np.array(dset.iloc[:,0])\n",
    "#\n",
    "# clf = MLPClassifier().fit(X_src, Y_src)\n",
    "#\n",
    "y_test_pred = clf.predict(X_src)\n",
    "y_test_score = clf.predict_proba(X_src) \n",
    "y_test_pred = to_categorical(y_test_score.argmax(axis=1))\n",
    "y_test_pred = y_test_pred.argmax(axis=1)\n",
    "y_test_score = y_test_score[:,1] \n",
    "#\n",
    "print('CM: ', confusion_matrix(Y_src, y_test_pred, labels=[0,1]).ravel())\n",
    "## Test Measures\n",
    "t_acc, t_sen, t_spe, t_f1, t_mcc, t_bacc, t_yi, t_auc, t_aupr = Calculate_Stats(Y_src,y_test_pred, y_test_score)\n",
    "print('t_acc, t_sen, t_spe, t_f1, t_mcc, t_bacc, t_yi, t_auc, t_aupr: ', t_acc, t_sen, t_spe, t_f1, t_mcc, t_bacc, t_yi, t_auc, t_aupr )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perf_raw_pbno3_1.groupby('Model').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perf_raw_pbno3_2.groupby('Model').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([perf_raw_pbno3_1, perf_raw_pbno3_2], axis=0).groupby('Model').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perf_psn_pbno3_1 = get_all_clf_res(pbno3_b1_psn, pbno3_b2_psn, 'pbno3_b1b2', 'psn')\n",
    "# perf_psn_pbno3_2 = get_all_clf_res(pbno3_b2_psn, pbno3_b1_psn, 'pbno3_b2b1', 'psn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perf_psn_pbno3_1.groupby('Model').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perf_psn_pbno3_2.groupby('Model').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.concat([perf_psn_pbno3_1, perf_psn_pbno3_2], axis=0).groupby('Model').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perf_bc_pbno3_1 = get_all_clf_res(pbno3_b1_bc, pbno3_b2_bc, 'pbno3_b1b2', 'bc')\n",
    "# perf_bc_pbno3_2 = get_all_clf_res(pbno3_b2_bc, pbno3_b1_bc, 'pbno3_b2b1', 'bc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perf_bc_pbno3_1.groupby('Model').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perf_bc_pbno3_2.groupby('Model').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.concat([perf_bc_pbno3_1, perf_bc_pbno3_2], axis=0).groupby('Model').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perf_bn_pbno3_1 = get_all_clf_res(pbno3_b1_bn, pbno3_b2_bn, 'pbno3_b1b2', 'bn')\n",
    "# perf_bn_pbno3_2 = get_all_clf_res(pbno3_b2_bn, pbno3_b1_bn, 'pbno3_b2b1', 'bn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perf_bn_pbno3_1.groupby('Model').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perf_bn_pbno3_2.groupby('Model').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.concat([perf_bn_pbno3_1, perf_bn_pbno3_2], axis=0).groupby('Model').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perf_bc_psn_pbno3_1 = get_all_clf_res(pbno3_b1_bl_psn, pbno3_b2_bl_psn, 'pbno3_b1b2', 'bc_psn')\n",
    "# perf_bc_psn_pbno3_2 = get_all_clf_res(pbno3_b2_bl_psn, pbno3_b1_bl_psn, 'pbno3_b2b1', 'bc_psn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perf_bc_psn_pbno3_1.groupby('Model').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perf_bc_psn_pbno3_2.groupby('Model').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.concat([perf_bc_psn_pbno3_1, perf_bc_psn_pbno3_2], axis=0).groupby('Model').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perf_bc_bn_pbno3_1 = get_all_clf_res(pbno3_b1_bl_bn, pbno3_b2_bl_bn, 'pbno3_b1b2', 'bc_bn')\n",
    "# perf_bc_bn_pbno3_2 = get_all_clf_res(pbno3_b2_bl_bn, pbno3_b1_bl_bn, 'pbno3_b2b1', 'bc_bn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perf_bc_bn_pbno3_1.groupby('Model').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perf_bc_bn_pbno3_2.groupby('Model').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.concat([perf_bc_bn_pbno3_1, perf_bc_bn_pbno3_2], axis=0).groupby('Model').mean()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fdddbd4c7a8b8735126cf4352355f5d9c26e14e7335a2014c33a9e391355e7b8"
  },
  "kernelspec": {
   "display_name": "sers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
